# -*- coding: utf-8 -*-
"""stockM sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oy-WGL6o0SiLDLUlvWgyoqR2uFRwn0Am
"""

from urllib.request import urlopen, Request
from bs4 import BeautifulSoup
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd
import matplotlib.pyplot as plt

finviz_url = 'https://finviz.com/quote.ashx?t='
tickers = ['AMZN', 'GOOG', 'MSFT']

news_tables = {}
for ticker in tickers:
    url = finviz_url + ticker

    req = Request(url=url, headers={'user-agent': 'my-app'})
    response = urlopen(req)

    html = BeautifulSoup(response, features='html.parser')
    news_table = html.find(id='news-table')
    news_tables[ticker] = news_table

news_tables.items()

from datetime import datetime

parsed_data = []

for ticker, news_table in news_tables.items():
    for row in news_table.findAll('tr'):

        # Extract the title
        if row.a:
            title = row.a.text.strip()  # Clean the title of extra whitespace
        else:
            title = ""

        # Extract the date and time from the row's first td element
        if row.td:
            date_data = row.td.text.strip()

            # Debugging: print the raw date_data for each row
            print(f"Raw date_data: '{date_data}'")

            # Split the date_data on spaces if it's available
            date_data_parts = date_data.split(' ')

            if len(date_data_parts) == 1:
                # Only time is available, set date to empty
                time = date_data_parts[0]
                date = ""
            elif len(date_data_parts) == 2:
                # Date and time are available
                date = date_data_parts[0]
                time = date_data_parts[1]
            else:
                # If there's no date or time, set both to empty
                date = time = ""
        else:
            date = time = ""

        # Append the parsed data to the list
        parsed_data.append([ticker, date, time, title])

# Print the parsed_data to verify the result
print(parsed_data)

from datetime import datetime, timedelta

# Assuming parsed_data is a list of lists, where each inner list is [ticker, date, ...other fields]

# Get today's date
today_date = datetime.today().strftime('%Y-%m-%d')

# Sort parsed_data by ticker to group entries for the same ticker together (optional)
parsed_data.sort(key=lambda x: x[0])  # Sort by ticker

# Initialize a dictionary to track the last date used for each ticker
ticker_dates = {}

# Step 1: Fill missing dates sequentially for each ticker
start_date = datetime.today()  # Today's date for starting the filling

for idx, row in enumerate(parsed_data):
    ticker = row[0]
    date = row[1]

    if date == "":  # If date is missing, assign a sequential date
        # Get the last date used for this ticker
        if ticker not in ticker_dates:
            ticker_dates[ticker] = start_date  # Start from today for each ticker

        # Assign the same date for all rows of the same ticker
        current_date = ticker_dates[ticker]
        row[1] = current_date.strftime('%Y-%m-%d')  # Format the date as 'YYYY-MM-DD'

        # Update the last date for this ticker (increment by 1 day)
        ticker_dates[ticker] += timedelta(days=1)

    elif date == "Today":  # Replace "Today" with today's actual date
        row[1] = today_date

# Print the updated parsed_data to verify the result
for row in parsed_data:
    print(row)

df = pd.DataFrame(parsed_data, columns=['ticker', 'date', 'time', 'title'])
df

import nltk
nltk.download('vader_lexicon')

vader = SentimentIntensityAnalyzer()



f = lambda title: vader.polarity_scores(title)['compound']
df['compound'] = df['title'].apply(f)
df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True, errors='coerce')

df

# Display data types of all columns in the DataFrame
print(df.dtypes)

df['time'] = pd.to_datetime(df['time'], infer_datetime_format=True, errors='coerce')
df['time_numeric'] = df['time'].dt.hour * 3600 + df['time'].dt.minute * 60 + df['time'].dt.second
df['time'] = df['time'].dt.strftime('%H:%M:%S')

# Display data types of all columns in the DataFrame
print(df.dtypes)

df







import matplotlib.pyplot as plt

# Group by 'ticker' and 'time_numeric', then compute the mean only for the 'compound' column
mean_df = df.groupby(['date', 'ticker'])['compound'].mean().unstack()
# Plotting the data
plt.figure(figsize=(200,120))
mean_df.plot(kind='bar')
plt.title('Average Compound Sentiment Score by Ticker and Time')
plt.xlabel('Ticker and Time')
plt.xticks(rotation=45, ha='right')

plt.ylabel('Average Compound Sentiment')
plt.show()

last_10_days = df['date'].drop_duplicates().sort_values(ascending=False).head(20)

# Filter the data to include only rows from these last 10 days
filtered_df = df[df['date'].isin(last_10_days)]

# Group by 'date' and 'ticker' to calculate the mean of 'compound' for each day
mean_df = filtered_df.groupby(['date', 'ticker'])['compound'].mean().unstack()

# Plotting the data
plt.figure(figsize=(20,12))
mean_df.plot(kind='bar')

# Set the title and labels
plt.title('Average Compound Sentiment Score by Ticker for the Last 10 Days')
plt.xlabel('Date')
plt.ylabel('Average Compound Sentiment')

# Rotate the x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

# Display the plot
plt.show()

